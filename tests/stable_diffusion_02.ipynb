{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' n�o � reconhecido como um comando interno\n",
      "ou externo, um programa oper�vel ou um arquivo em lotes.\n",
      "'wget' n�o � reconhecido como um comando interno\n",
      "ou externo, um programa oper�vel ou um arquivo em lotes.\n"
     ]
    }
   ],
   "source": [
    "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/examples/dreambooth/train_dreambooth.py\n",
    "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/scripts/convert_diffusers_to_original_stable_diffusion.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qq git+https://github.com/ShivamShrirao/diffusers\n",
    "%pip install -q -U --pre triton\n",
    "%pip install -q accelerate transformers ftfy bitsandbytes==0.35.0 gradio natsort safetensors xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sd = \"runwayml/stable-diffusion-v1-5\"\n",
    "output_dir = \"/content/stable_diffusion_weights/zwx\"\n",
    "\n",
    "print(f\"Diretório: {output_dir}\")\n",
    "!mkdir -p $output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts_list = [\n",
    "    {\n",
    "        \"instance_prompt\": \"zwx\",\n",
    "        \"class_prompt\": \"photo of a person\",\n",
    "        \"instance_data_dir\": \"/content/data/zwx\",\n",
    "        \"class_data_dir\": \"/content/data/person\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "for c in concepts_list:\n",
    "  os.makedirs(c[\"instance_data_dir\"], exist_ok=True)\n",
    "\n",
    "with open(\"concepts_list.json\", \"w\") as f:\n",
    "  json.dump(concepts_list, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 train_dreambooth.py \\\n",
    "  --pretrained_model_name_or_path=$model_sd \\\n",
    "  --pretrained_vae_name_or_path=\"stabilityai/sd-vae-ft-mse\" \\\n",
    "  --output_dir=$output_dir \\\n",
    "  --revision=\"fp16\" \\\n",
    "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
    "  --seed=777 \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=1 \\\n",
    "  --train_text_encoder \\\n",
    "  --mixed_precision=\"fp16\" \\\n",
    "  --use_8bit_adam \\\n",
    "  --gradient_accumulation_steps=1 \\\n",
    "  --learning_rate=$learning_rate \\\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=80 \\\n",
    "  --num_class_images=$num_class_images \\\n",
    "  --sample_batch_size=4 \\\n",
    "  --max_train_steps=$max_num_steps \\\n",
    "  --save_interval=10000 \\\n",
    "  --save_sample_prompt=\"zwx\" \\\n",
    "  --concepts_list=\"concepts_list.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natsort import natsorted\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "weights_dir = natsorted(glob(output_dir + os.sep + \"*\"))[-1]\n",
    "print(f\"Diretório com os pesos: {weights_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "def grid_img(imgs, rows=1, cols=3, scale=1):\n",
    "  assert len(imgs) == rows * cols\n",
    "\n",
    "  w, h = imgs[0].size\n",
    "  w, h = int(w*scale), int(h*scale)\n",
    "\n",
    "  grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "  grid_w, grid_h = grid.size\n",
    "\n",
    "  for i, img in enumerate(imgs):\n",
    "      img = img.resize((w,h), Image.ANTIALIAS)\n",
    "      grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "  return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_folder = output_dir\n",
    "folders = sorted([f for f in os.listdir(weights_folder) if f != \"0\"], key = lambda x: int(x))\n",
    "#print(folders)\n",
    "\n",
    "imgs_test = []\n",
    "\n",
    "for imgs, folder in enumerate(folders):\n",
    "  #print(folder)\n",
    "  folder_path = os.path.join(weights_folder, folder)\n",
    "  image_folder = os.path.join(folder_path, \"samples\")\n",
    "  images = [f for f in os.listdir(image_folder)]\n",
    "\n",
    "  for i in images:\n",
    "    img_path = os.path.join(image_folder, i)\n",
    "    r = Image.open(img_path)\n",
    "    imgs_test.append(r)\n",
    "\n",
    "grid_img(imgs_test, rows=1, cols=4, scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = weights_dir + \"/model.ckpt\"\n",
    "\n",
    "half_arg = \"--half\"\n",
    "\n",
    "!python convert_diffusers_to_original_stable_diffusion.py --model_path $weights_dir  --checkpoint_path $ckpt_path $half_arg\n",
    "print(f\"Converted to ckpt and saved in {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import autocast\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = weights_dir\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = StableDiffusionPipeline.from_pretrained(model_path, torch_dtype=torch.float16).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.enable_xformers_memory_efficient_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 777"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"face portrait of zwx in the snow, realistic, hd, vivid, sunset\"\n",
    "negative_prompt = \"bad anatomy, ugly, deformed, desfigured, distorted face, poorly drawn hands, poorly drawn face, poorly drawn feet, blurry, low quality, low definition, lowres, out of frame, out of image, cropped, cut off, signature, watermark\"\n",
    "num_samples = 5\n",
    "guidance_scale = 7.5\n",
    "num_inference_steps = 30\n",
    "height = 512\n",
    "width = 512\n",
    "\n",
    "seed = random.randint(0, 2147483647) # gera um valor aleatório\n",
    "print(\"Seed: {}\".format(str(seed)))\n",
    "generator = torch.Generator(device='cuda').manual_seed(seed)\n",
    "\n",
    "with autocast(\"cuda\"), torch.inference_mode():\n",
    "    imgs = pipe(\n",
    "        prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        height=height, width=width,\n",
    "        num_images_per_prompt=num_samples,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        generator=generator\n",
    "    ).images\n",
    "\n",
    "for img in imgs:\n",
    "    display(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
